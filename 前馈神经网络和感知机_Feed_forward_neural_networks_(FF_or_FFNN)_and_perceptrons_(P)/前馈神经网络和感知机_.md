  
  
- [Ch1 前馈神经网络和感知机](#ch1-前馈神经网络和感知机 )
  - [前馈神经网络](#前馈神经网络 )
    - [单层前馈神经网络](#单层前馈神经网络 )
    - [多层前馈神经网络](#多层前馈神经网络 )
  - [BP神经网络](#bp神经网络 )
    - [反向传播](#反向传播 )
    - [梯度下降](#梯度下降 )
  - [感知机](#感知机 )
    - [感知机概述](#感知机概述 )
    - [感知机的数学模型](#感知机的数学模型 )
    - [感知机的几何解释](#感知机的几何解释 )
    - [感知机的学习过程](#感知机的学习过程 )
  
##  Ch1 前馈神经网络和感知机
  
先介绍最简单、最原始的神经网络——前馈神经网络的概念，再讲解前馈神经网络的训练过程——反向传播（BP）和梯度下降，最后通过简单的实例——感知机解释前馈神经网络的一种应用
  
###  前馈神经网络
  
  
每一层神经元接受前一层神经元的输出作为输入，并输出到下一层神经元。网络中无反馈，信号单向传播。单独一层可以抽象为一个线性函数<img src="https://latex.codecogs.com/gif.latex?&#x5C;%20y%20=%20&#x5C;omega%20x%20+%20b"/>。通过多个这样的层，可以实现输入空间到输出空间的复杂映射
  
####  单层前馈神经网络
  
不考虑激活函数，<img src="https://latex.codecogs.com/gif.latex?x"/> 为输入（input），<img src="https://latex.codecogs.com/gif.latex?&#x5C;theta"/> 为权重（weight）。通常 <img src="https://latex.codecogs.com/gif.latex?x_{1}"/> 固定为 1，对应的 <img src="https://latex.codecogs.com/gif.latex?&#x5C;theta_{1}"/> 称为偏置项
  
- 图像表示
<img src="img/单层前馈神经网络.png">
  
- 矩阵表示
<img src="https://latex.codecogs.com/gif.latex?y"/> 和 <img src="https://latex.codecogs.com/gif.latex?x"/> 满足这样的关系式
<p align="center"><img src="https://latex.codecogs.com/gif.latex?y%20=%20x_{1}*&#x5C;theta_{1}+x_{2}*&#x5C;theta_{2}+&#x5C;cdots%20=%20&#x5C;sum%20x_{n}*&#x5C;theta_{n}"/></p>  
  
表示为矩阵形式也就是
<p align="center"><img src="https://latex.codecogs.com/gif.latex?x%20=%20&#x5C;begin{Bmatrix}%20%20x_{1}%20&#x5C;&#x5C;%20x_{2}%20&#x5C;&#x5C;%20x_{3}%20&#x5C;&#x5C;%20&#x5C;vdots%20&#x5C;end{Bmatrix}%20&#x5C;%20&#x5C;%20&#x5C;%20&#x5C;theta=&#x5C;begin{Bmatrix}%20&#x5C;theta_{1}%20&#x5C;&#x5C;%20&#x5C;theta_{2}%20&#x5C;&#x5C;%20&#x5C;theta_{3}%20&#x5C;&#x5C;%20&#x5C;vdots%20&#x5C;end{Bmatrix}%20&#x5C;%20&#x5C;&#x5C;%20&#x5C;%20&#x5C;&#x5C;%20y%20=%20&#x5C;theta^{T}x"/></p>  
  
  
####  多层前馈神经网络
  
第零层称为输入层，最后一层称为输出层，中间其他层称为隐藏层
  
- 图像表示
<img src="img/多层前馈神经网络.png">
  
- 矩阵表示
<img src="https://latex.codecogs.com/gif.latex?y"/> 和 <img src="https://latex.codecogs.com/gif.latex?x"/> 满足这样的关系式
<p align="center"><img src="https://latex.codecogs.com/gif.latex?y_{1}%20=%20x_{1}*&#x5C;theta_{11}+x_{2}*&#x5C;theta_{21}+&#x5C;cdots%20=%20&#x5C;sum%20x_{n}*&#x5C;theta_{n1}&#x5C;&#x5C;%20y_{2}%20=%20x_{1}*&#x5C;theta_{12}+x_{2}*&#x5C;theta_{22}+&#x5C;cdots%20=%20&#x5C;sum%20x_{n}*&#x5C;theta_{n2}&#x5C;&#x5C;&#x5C;cdots&#x5C;&#x5C;y_{n}%20=%20x_{1}*&#x5C;theta_{1n}+x_{2}*&#x5C;theta_{2n}+&#x5C;cdots%20=%20&#x5C;sum%20x_{n}*&#x5C;theta_{nn}"/></p>  
  
表示为矩阵形式也就是
<p align="center"><img src="https://latex.codecogs.com/gif.latex?x%20=%20&#x5C;begin{Bmatrix}%20%20x_{1}%20&#x5C;&#x5C;%20x_{2}%20&#x5C;&#x5C;%20x_{3}%20&#x5C;&#x5C;%20&#x5C;vdots%20&#x5C;end{Bmatrix}%20&#x5C;%20&#x5C;%20&#x5C;%20&#x5C;theta=&#x5C;begin{Bmatrix}%20&#x5C;theta_{11}%20&amp;&amp;%20&#x5C;theta_{12}%20&amp;&amp;%20&#x5C;theta_{13}%20&amp;&amp;%20&#x5C;cdots%20&amp;&amp;%20&#x5C;theta_{1n}%20&#x5C;&#x5C;%20&#x5C;theta_{21}%20&amp;&amp;%20&#x5C;theta_{22}%20&amp;&amp;%20&#x5C;theta_{23}%20&amp;&amp;%20&#x5C;cdots%20&amp;&amp;%20&#x5C;theta_{2n}%20&#x5C;&#x5C;%20&#x5C;theta_{31}%20&amp;&amp;%20&#x5C;theta_{32}%20&amp;&amp;%20&#x5C;theta_{33}%20&amp;&amp;%20&#x5C;cdots%20&amp;&amp;%20&#x5C;theta_{3n}%20&#x5C;&#x5C;%20&#x5C;vdots%20&#x5C;end{Bmatrix}%20&#x5C;%20&#x5C;%20&#x5C;%20y=&#x5C;begin{Bmatrix}%20y_{1}%20&#x5C;&#x5C;%20y_{2}%20&#x5C;&#x5C;%20y_{3}%20&#x5C;&#x5C;%20&#x5C;vdots%20&#x5C;end{Bmatrix}%20&#x5C;&#x5C;%20&#x5C;%20&#x5C;&#x5C;%20y%20=%20&#x5C;theta^{T}x"/></p>  
  
<img src="https://latex.codecogs.com/gif.latex?z"/> 和 <img src="https://latex.codecogs.com/gif.latex?y"/> 的关系与单层前馈神经网络相同
  
###  BP神经网络
  
用反向传播算法（BP）训练多层前馈神经网络，包括了信号前向传播和误差反向传播两个过程。BP 算法的指导思想是梯度下降（<img src="https://latex.codecogs.com/gif.latex?&#x5C;omega%20=%20&#x5C;omega%20-&#x5C;alpha%20*%20&#x5C;delta%20&#x5C;omega"/>）
  
####  反向传播
  
通过链式法则计算损失函数对每一个参数的导数
  
<img src="img/反向传播.png">
  
根据前馈神经网络输入和输出的关系，我们可以得到这样的信息
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?y%20=%20&#x5C;omega_{1}*x+b_{1}%20&#x5C;&#x5C;z%20=%20&#x5C;omega_{2}*y+b_{2}"/></p>  
  
  
假设损失函数是根据 <img src="https://latex.codecogs.com/gif.latex?z"/> 定义的 <img src="https://latex.codecogs.com/gif.latex?Loss(z)"/>，为了找到能够最小化损失函数的参数 <img src="https://latex.codecogs.com/gif.latex?&#x5C;omega"/> 和 <img src="https://latex.codecogs.com/gif.latex?b"/>，就需要求出 <img src="https://latex.codecogs.com/gif.latex?Loss(z)"/> 对 <img src="https://latex.codecogs.com/gif.latex?&#x5C;omega"/> 和 <img src="https://latex.codecogs.com/gif.latex?b"/> 的偏导数来确定梯度下降的方向
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?&#x5C;frac{&#x5C;delta%20Loss(z)}{&#x5C;delta%20&#x5C;omega_{2}}%20=%20Loss&#x27;(z)*y%20&#x5C;%20&#x5C;&#x5C;%20&#x5C;%20&#x5C;&#x5C;%20&#x5C;%20&#x5C;frac{&#x5C;delta%20Loss(z)}{&#x5C;delta%20b_{2}}%20=%20Loss&#x27;(z)*1%20&#x5C;&#x5C;%20&#x5C;%20&#x5C;&#x5C;%20&#x5C;frac{&#x5C;delta%20Loss(z)}{&#x5C;delta%20&#x5C;omega_{1}}%20=%20Loss&#x27;(z)*&#x5C;frac{&#x5C;delta%20z}{&#x5C;delta%20y}*&#x5C;frac{&#x5C;delta%20y}{&#x5C;delta%20&#x5C;omega_{1}}%20=%20Loss&#x27;(z)*&#x5C;omega_{2}*x%20&#x5C;&#x5C;%20&#x5C;%20&#x5C;&#x5C;%20&#x5C;frac{&#x5C;delta%20Loss(z)}{&#x5C;delta%20b_{1}}%20=%20Loss&#x27;(z)*&#x5C;frac{&#x5C;delta%20z}{&#x5C;delta%20y}*&#x5C;frac{&#x5C;delta%20y}{b_{1}}%20=%20Loss&#x27;(z)*&#x5C;omega_{2}"/></p>  
  
  
使用链式求导的方法，计算损失函数对每个参数的偏导数的过程就是反向传播
  
####  梯度下降
  
  
顾名思义，梯度下降就是沿着梯度向下的方向调整参数，以达到使目标函数取到极小值的目标
  
因为实数的分布是连续的，而实现梯度下降的过程是离散的，因此只能按照一定的步长逐步逼近损失函数极小值，即按照 <img src="https://latex.codecogs.com/gif.latex?a_{k+1}%20=%20a_{k}+&#x5C;rho_{k}%20&#x5C;vec{s}"/> 的规律不断更新参数以逼近极小值点
  
其中 <img src="https://latex.codecogs.com/gif.latex?&#x5C;rho"/> 称为步长，就是参数朝梯度方向移动的距离，过大的步长会导致函数值发散，过小的步长又会导致函数值迟迟不能取到极小值，因此应该对每个参数 <img src="https://latex.codecogs.com/gif.latex?a_{k}"/> 选择合适的 <img src="https://latex.codecogs.com/gif.latex?&#x5C;rho_{k}"/>
  
根据梯度下降的样本选取方式，分为三种
- 批量梯度下降（Batch Gradient Descent，BGD）
每次迭代使用梯度下降中，取所有的样本计算出来的数据来更新参数，优点是充分利用全数据集，缺点是训练过程很慢
- 随机梯度下降（Stochastic Gradient Descent，SGD）
每次迭代使用梯度下降中，随机取一个样本来更新参数，优点是训练速度加快，缺点是准确度下降
- 小批量梯度下降（Mini-Batch Gradient Descent，MBGD）
每次迭代使用梯度下降中，取 <img src="https://latex.codecogs.com/gif.latex?batch&#x5C;_size"/> 个样本来更新参数，平衡 BGD 和 SGD 的优缺点
  
###  感知机
  
  
####  感知机概述
  
感知机是用于二分类的线性分类模型，输入实例的特征向量，输出实例的类别（取值为±1）
  
感知机和输入空间中将实例划分为正负两类的超平面相对应，属于一种判别模型
  
####  感知机的数学模型
  
假设输入空间 <img src="https://latex.codecogs.com/gif.latex?x%20&#x5C;subseteq%20R^{n}"/>，输出空间 <img src="https://latex.codecogs.com/gif.latex?y%20=%20&#x5C;{-1,+1&#x5C;}"/>。感知机即为输入空间到输出空间的如下函数
<p align="center"><img src="https://latex.codecogs.com/gif.latex?y%20=%20sign(&#x5C;omega*x+b)=&#x5C;begin{cases}%20+1,&#x5C;omega*x+b&#x5C;ge%200%20&#x5C;&#x5C;%20-1,&#x5C;omega*x+b&#x5C;lt0%20&#x5C;end{cases}"/></p>  
  
  
<img src="https://latex.codecogs.com/gif.latex?&#x5C;omega"/> 为权值（weight），<img src="https://latex.codecogs.com/gif.latex?b"/> 为偏置（bias）
  
####  感知机的几何解释
  
线性方程<img src="https://latex.codecogs.com/gif.latex?&#x5C;%20&#x5C;omega%20*x+b=0"/>&ensp;对应于特征空间 <img src="https://latex.codecogs.com/gif.latex?R^{n}"/> 中的一个超平面，其中 <img src="https://latex.codecogs.com/gif.latex?&#x5C;omega"/> 对应的是法向量，<img src="https://latex.codecogs.com/gif.latex?b"/> 对应的是截距
  
任何特征空间上的实例，会落在超平面的两侧，以此为依据可以划分为正、负两类
  
<img src="img/感知机几何解释.png">
  
####  感知机的学习过程
  
  
假设已有训练数据集 <img src="https://latex.codecogs.com/gif.latex?T=&#x5C;{(x_{1},y_{1}),(x_{2},y_{2}),(x_{3},y_{3})&#x5C;cdots%20,(x_{n},y_{n})&#x5C;}"/>
  
求解最合适的参数 <img src="https://latex.codecogs.com/gif.latex?&#x5C;omega"/> 和 <img src="https://latex.codecogs.com/gif.latex?b"/> 的问题，等效于求损失函数极小值 <img src="https://latex.codecogs.com/gif.latex?min(Loss(&#x5C;omega,b))%20=%20min(-&#x5C;sum%20y_{i}(&#x5C;omega%20*x_{i}+b))"/>
  
损失函数 <img src="https://latex.codecogs.com/gif.latex?Loss(&#x5C;omega,b)%20=%20-&#x5C;sum%20y_{i}(&#x5C;omega%20*x_{i}+b)"/>
- 假设分类正确，<img src="https://latex.codecogs.com/gif.latex?y_{i}"/> 和 <img src="https://latex.codecogs.com/gif.latex?&#x5C;omega%20*%20x_{i}%20+b"/> 符号相同，则 <img src="https://latex.codecogs.com/gif.latex?-y_{i}(&#x5C;omega%20*x_{i}+b)"/> 使得 <img src="https://latex.codecogs.com/gif.latex?Loss(&#x5C;omega,%20b)"/> 减小
- 假设分类错误，<img src="https://latex.codecogs.com/gif.latex?y_{i}"/> 和 <img src="https://latex.codecogs.com/gif.latex?&#x5C;omega%20*%20x_{i}%20+b"/> 符号不同，则 <img src="https://latex.codecogs.com/gif.latex?-y_{i}(&#x5C;omega%20*x_{i}+b)"/> 使得 <img src="https://latex.codecogs.com/gif.latex?Loss(&#x5C;omega,%20b)"/> 增大
  
采用随机梯度下降的方式，每次选出一个误分类点来更新 <img src="https://latex.codecogs.com/gif.latex?&#x5C;omega"/> 和 <img src="https://latex.codecogs.com/gif.latex?b"/>
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?&#x5C;frac{&#x5C;delta%20Loss(&#x5C;omega,b)}{&#x5C;delta%20&#x5C;omega}=-y_{i}x_{i}&#x5C;%20&#x5C;%20&#x5C;%20&#x5C;%20&#x5C;omega_{k+1}%20=%20&#x5C;omega_{k}%20+%20&#x5C;lambda*y_{i}x_{i}&#x5C;&#x5C;&#x5C;%20&#x5C;&#x5C;&#x5C;frac{&#x5C;delta%20Loss(&#x5C;omega,b)}{&#x5C;delta%20b}=-y_{i}&#x5C;%20&#x5C;%20&#x5C;%20&#x5C;%20&#x5C;omega_{k+1}%20=%20&#x5C;omega_{k}%20+%20&#x5C;lambda*y_{i}"/></p>  
  
  
不断迭代，直到损失函数减小到一定的值，就可以视为学习完成
  